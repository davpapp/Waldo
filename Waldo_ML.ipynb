{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in /home/dp/anaconda3/lib/python3.7/site-packages (0.0.1)\r\n",
      "Requirement already satisfied: torch in /home/dp/anaconda3/lib/python3.7/site-packages (from torchviz) (1.2.0)\r\n",
      "Requirement already satisfied: graphviz in /home/dp/anaconda3/lib/python3.7/site-packages (from torchviz) (0.13)\r\n",
      "Requirement already satisfied: numpy in /home/dp/anaconda3/lib/python3.7/site-packages (from torch->torchviz) (1.16.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # we always love numpy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "waldo_images = glob.glob(\"./Waldo/64/waldo/*.jpg\")\n",
    "not_waldo_images = glob.glob(\"./Waldo/64/notwaldo/*.jpg\")\n",
    "\n",
    "labels = []\n",
    "for image in waldo_images:\n",
    "    labels.append((image, 0))\n",
    "for image in not_waldo_images:\n",
    "    labels.append([image, 1])\n",
    "a = np.asarray(labels)\n",
    "pd.DataFrame(a).to_csv(\"C:/Users/nshuster/Desktop/labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/nshuster/Desktop/Hey-Waldo/64/notwaldo\\10_10_4.jpg\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "csv_frame = pd.read_csv(\"C:/Users/nshuster/Desktop/labels.csv\")\n",
    "n = 65\n",
    "image_name = csv_frame.iloc[n, 1]\n",
    "image_label = csv_frame.iloc[n, 2]\n",
    "\n",
    "print(image_name)\n",
    "print(image_label)\n",
    "\n",
    "def show_label(image, label):\n",
    "    plt.imshow(image)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.figure()\n",
    "show_label(mpimg.imread(image_name), image_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import io, transform\n",
    "\n",
    "class WaldoDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.root_dir,self.annotations_frame.iloc[idx, 1])\n",
    "        image = io.imread(img_name)\n",
    "        label = self.annotations_frame.iloc[idx, 2]\n",
    "        label = label.astype('long')\n",
    "        #annotations = self.annotations_frame.iloc[idx, 2]\n",
    "        #annotations = np.array([annotations])\n",
    "        #annotations = annotations.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "            'label': torch.tensor(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.WaldoDataset object at 0x000001C45C517A58>\n"
     ]
    }
   ],
   "source": [
    "waldo_dataset = WaldoDataset(csv_file=\"C:/Users/nshuster/Desktop/labels.csv\", root_dir=\"C:/Users/nshuster/Desktop/Hey-Waldo/64/\", transform=ToTensor())\n",
    "fig = plt.figure()\n",
    "\n",
    "print(waldo_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_dims = [3, 64, 64]\n",
    "classes = ('notwaldo', 'waldo')\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "    # The init funciton in Pytorch classes is used to keep track of the parameters of the model\n",
    "    # specifically the ones we want to update with gradient descent + backprop\n",
    "    # So we need to make sure we keep track of all of them here\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        # layers defined here\n",
    "\n",
    "        # Make sure you understand what this convolutional layer is doing.\n",
    "        # E.g., considering looking at help(nn.Conv2D).  Draw a picture of what\n",
    "        # this layer does to the data.\n",
    "\n",
    "        # note: image_dims[0] will be 3 as there are 3 color channels (R, G, B)\n",
    "        num_kernels = 16\n",
    "        self.conv1 = nn.Conv2d(image_dims[0], num_kernels, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "        # Make sure you understand what this MaxPool2D layer is doing.\n",
    "        # E.g., considering looking at help(nn.MaxPool2D).  Draw a picture of\n",
    "        # what this layer does to the data.\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # maxpool_output_size is the total amount of data coming out of that\n",
    "        # layer.  We have an exercise that asks you to explain why the line of\n",
    "        # code below computes this quantity.\n",
    "        self.maxpool_output_size = int(num_kernels * (image_dims[1] / 2) * (image_dims[2] / 2))\n",
    "\n",
    "        # Add on a fully connected layer (like in our MLP)\n",
    "        # fc stands for fully connected\n",
    "        fc1_size = 64\n",
    "        self.fc1 = nn.Linear(self.maxpool_output_size, fc1_size)\n",
    "\n",
    "        # we'll use this activation function internally in the network\n",
    "        self.activation_func = torch.nn.ReLU()\n",
    "\n",
    "        # Convert our fully connected layer into outputs that we can compare to the result\n",
    "        fc2_size = len(classes)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "\n",
    "        # Note: that the output will not represent the probability of the\n",
    "        # output being in each class.  The loss function we will use\n",
    "        # `CrossEntropyLoss` will take care of convering these values to\n",
    "        # probabilities and then computing the log loss with respect to the\n",
    "        # true label.  We could break this out into multiple steps, but it turns\n",
    "        # out that the algorithm will be more numerically stable if we do it in\n",
    "        # one go.  We have included a cell to show you the documentation for\n",
    "        # `CrossEntropyLoss` if you'd like to check it out.\n",
    "        \n",
    "    # The forward function in the class defines the operations performed on a given input to the model\n",
    "    # and returns the output of the model\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.activation_func(x)\n",
    "        # this code flattens the output of the convolution, max pool,\n",
    "        # activation sequence of steps into a vector\n",
    "        x = x.view(-1, self.maxpool_output_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_func(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    # The loss function (which we chose to include as a method of the class, but doesn't need to be)\n",
    "    # returns the loss and optimizer used by the model\n",
    "    def get_loss(self, learning_rate):\n",
    "      # Loss function\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "      # Optimizer, self.parameters() returns all the Pytorch operations that are attributes of the class\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return loss, optimizer\n",
    "\n",
    "# Define what device we want to use\n",
    "#device = 'cuda' # 'cpu' if we want to not use the gpu\n",
    "# Initialize the model, loss, and optimization function\n",
    "net = MyCNN()\n",
    "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
    "#net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"print(type(waldo_dataset))\n",
    "reduced_waldo_dataset = []\n",
    "for i in range(100):\n",
    "  reduced_waldo_dataset.append(waldo_dataset[i])\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"reduced_train_size = int(0.75 * len(reduced_waldo_dataset))\n",
    "reduced_test_size = len(reduced_waldo_dataset) - reduced_train_size\n",
    "reduced_waldo_dataset_train, reduced_waldo_dataset_test = torch.utils.data.random_split(reduced_waldo_dataset, [reduced_train_size, reduced_test_size])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_size = int(0.05 * len(waldo_dataset))\n",
    "test_size = len(waldo_dataset) - train_size\n",
    "waldo_dataset_train, waldo_dataset_test = torch.utils.data.random_split(waldo_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(waldo_dataset_train, batch_size=4, num_workers=2)\n",
    "test_loader = DataLoader(waldo_dataset_test, batch_size=4)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "n_epochs = 2\n",
    "\n",
    "def train_model(net):\n",
    "    \"\"\" Train a the specified network.\n",
    "\n",
    "        Outputs a tuple with the following four elements\n",
    "        train_hist_x: the x-values (batch number) that the training set was \n",
    "            evaluated on.\n",
    "        train_loss_hist: the loss values for the training set corresponding to\n",
    "            the batch numbers returned in train_hist_x\n",
    "        test_hist_x: the x-values (batch number) that the test set was \n",
    "            evaluated on.\n",
    "        test_loss_hist: the loss values for the test set corresponding to\n",
    "            the batch numbers returned in test_hist_x\n",
    "    \"\"\" \n",
    "    loss, optimizer = net.get_loss(learning_rate)\n",
    "    # Define some parameters to keep track of metrics\n",
    "    print_every = 50\n",
    "    idx = 0\n",
    "    print(\"started training...\")\n",
    "    train_hist_x = []\n",
    "    train_loss_hist = []\n",
    "    test_hist_x = []\n",
    "    test_loss_hist = []\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    # Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"epoch %(e)d\" % {\"e\":epoch})\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "          #print(i, data)\n",
    "\n",
    "            # Get inputs in right form\n",
    "           # print(data)\n",
    "            #print(data['image']) #rint(data['inputs'])\n",
    "            inputs, labels = data['image'], data['label']\n",
    "            #print(inputs)\n",
    "          #print(labels)\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n",
    "          \n",
    "          # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "          # Forward pass\n",
    "            outputs = net(inputs)\n",
    "          \n",
    "            print(\"forward passed\")\n",
    "          # Compute the loss and find the loss with respect to each parameter of the model\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            print(\"calculate loss\")\n",
    "          # Change each parameter with respect to the recently computed loss.\n",
    "            optimizer.step()\n",
    "\n",
    "          # Update statistics\n",
    "            running_loss += loss_size.data.item()\n",
    "            print(\"updated loss\")\n",
    "          # Print every 20th batch of an epoch\n",
    "            if (i % print_every) == print_every-1:\n",
    "                print(\"Epoch {}, Iteration {}\\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                    epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
    "              # Reset running loss and time\n",
    "                train_loss_hist.append(running_loss / print_every)\n",
    "                train_hist_x.append(idx)\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "                idx += 1\n",
    "\n",
    "        # At the end of the epoch, do a pass on the test set\n",
    "            total_test_loss = 0\n",
    "            print(\"test set pass\")\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                inputs, labels = data['image'], data['label']\n",
    "\n",
    "            # Wrap tensors in Variables\n",
    "                inputs, labels = Variable(inputs).to(device, dtype=torch.float), Variable(labels).to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "                test_outputs = net(inputs)\n",
    "                test_loss_size = loss(test_outputs, labels)\n",
    "                total_test_loss += test_loss_size.data.item()\n",
    "            test_loss_hist.append(total_test_loss / len(test_loader))\n",
    "            test_hist_x.append(idx)\n",
    "            print(\"Validation loss = {:.2f}\".format(\n",
    "                total_test_loss / len(test_loader)))\n",
    "\n",
    "        print(\"Training finished, took {:.2f}s\".format(\n",
    "        time.time() - training_start_time))\n",
    "        return train_hist_x, train_loss_hist, test_hist_x, test_loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started training...\n",
      "epoch 0\n"
     ]
    }
   ],
   "source": [
    "train_hist_x, train_loss_hist, test_hist_x, test_loss_hist = train_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
