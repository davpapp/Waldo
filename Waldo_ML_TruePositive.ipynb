{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in c:\\programdata\\miniconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: torch in c:\\programdata\\miniconda3\\lib\\site-packages (from torchviz) (1.0.1)\n",
      "Requirement already satisfied: graphviz in c:\\programdata\\miniconda3\\lib\\site-packages (from torchviz) (0.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 19.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # we always love numpy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "1337\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "waldo_images = glob.glob(\"C:/Users/nshuster/Desktop/Hey-Waldo/64/waldo/*.jpg\")\n",
    "print(len(waldo_images))\n",
    "not_waldo_images = glob.glob(\"C:/Users/nshuster/Desktop/Hey-Waldo/64/notwaldo/*.jpg\")[4000:]\n",
    "\n",
    "print(len(not_waldo_images))\n",
    "labels = []\n",
    "for image in waldo_images:\n",
    "    labels.append((image, 0))\n",
    "for image in not_waldo_images:\n",
    "    labels.append([image, 1])\n",
    "a = np.asarray(labels)\n",
    "pd.DataFrame(a).to_csv(\"C:/Users/nshuster/Desktop/labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/nshuster/Desktop/Hey-Waldo/64/notwaldo\\4_7_15.jpg\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "csv_frame = pd.read_csv(\"C:/Users/nshuster/Desktop/labels.csv\")\n",
    "n = 65\n",
    "image_name = csv_frame.iloc[n, 1]\n",
    "image_label = csv_frame.iloc[n, 2]\n",
    "\n",
    "print(image_name)\n",
    "print(image_label)\n",
    "\n",
    "def show_label(image, label):\n",
    "    plt.imshow(image)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.figure()\n",
    "show_label(mpimg.imread(image_name), image_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import io, transform\n",
    "\n",
    "class WaldoDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.root_dir,self.annotations_frame.iloc[idx, 1])\n",
    "        image = io.imread(img_name)\n",
    "        label = self.annotations_frame.iloc[idx, 2]\n",
    "        label = label.astype('long')\n",
    "        #annotations = self.annotations_frame.iloc[idx, 2]\n",
    "        #annotations = np.array([annotations])\n",
    "        #annotations = annotations.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "            'label': torch.tensor(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.WaldoDataset object at 0x0000015F8AE2D6D8>\n"
     ]
    }
   ],
   "source": [
    "waldo_dataset = WaldoDataset(csv_file=\"C:/Users/nshuster/Desktop/labels.csv\", root_dir=\"C:/Users/nshuster/Desktop/Hey-Waldo/64/\", transform=ToTensor())\n",
    "fig = plt.figure()\n",
    "\n",
    "print(waldo_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=16384, out_features=64, bias=True)\n",
       "  (activation_func): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dims = [3, 64, 64]\n",
    "classes = ('notwaldo', 'waldo')\n",
    "print(len(waldo_dataset))\n",
    "class MyCNN(nn.Module):\n",
    "    # The init funciton in Pytorch classes is used to keep track of the parameters of the model\n",
    "    # specifically the ones we want to update with gradient descent + backprop\n",
    "    # So we need to make sure we keep track of all of them here\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        # layers defined here\n",
    "\n",
    "        # Make sure you understand what this convolutional layer is doing.\n",
    "        # E.g., considering looking at help(nn.Conv2D).  Draw a picture of what\n",
    "        # this layer does to the data.\n",
    "\n",
    "        # note: image_dims[0] will be 3 as there are 3 color channels (R, G, B)\n",
    "        num_kernels = 16\n",
    "        self.conv1 = nn.Conv2d(image_dims[0], num_kernels, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "        # Make sure you understand what this MaxPool2D layer is doing.\n",
    "        # E.g., considering looking at help(nn.MaxPool2D).  Draw a picture of\n",
    "        # what this layer does to the data.\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # maxpool_output_size is the total amount of data coming out of that\n",
    "        # layer.  We have an exercise that asks you to explain why the line of\n",
    "        # code below computes this quantity.\n",
    "        self.maxpool_output_size = int(num_kernels * (image_dims[1] / 2) * (image_dims[2] / 2))\n",
    "\n",
    "        # Add on a fully connected layer (like in our MLP)\n",
    "        # fc stands for fully connected\n",
    "        fc1_size = 64\n",
    "        self.fc1 = nn.Linear(self.maxpool_output_size, fc1_size)\n",
    "\n",
    "        # we'll use this activation function internally in the network\n",
    "        self.activation_func = torch.nn.ReLU()\n",
    "\n",
    "        # Convert our fully connected layer into outputs that we can compare to the result\n",
    "        fc2_size = len(classes)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "\n",
    "        # Note: that the output will not represent the probability of the\n",
    "        # output being in each class.  The loss function we will use\n",
    "        # `CrossEntropyLoss` will take care of convering these values to\n",
    "        # probabilities and then computing the log loss with respect to the\n",
    "        # true label.  We could break this out into multiple steps, but it turns\n",
    "        # out that the algorithm will be more numerically stable if we do it in\n",
    "        # one go.  We have included a cell to show you the documentation for\n",
    "        # `CrossEntropyLoss` if you'd like to check it out.\n",
    "        \n",
    "    # The forward function in the class defines the operations performed on a given input to the model\n",
    "    # and returns the output of the model\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.activation_func(x)\n",
    "        # this code flattens the output of the convolution, max pool,\n",
    "        # activation sequence of steps into a vector\n",
    "        x = x.view(-1, self.maxpool_output_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_func(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    # The loss function (which we chose to include as a method of the class, but doesn't need to be)\n",
    "    # returns the loss and optimizer used by the model\n",
    "    def get_loss(self, learning_rate):\n",
    "      # Loss function\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "      # Optimizer, self.parameters() returns all the Pytorch operations that are attributes of the class\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return loss, optimizer\n",
    "\n",
    "# Define what device we want to use\n",
    "device = 'cpu' # 'cpu' if we want to not use the gpu\n",
    "# Initialize the model, loss, and optimization function\n",
    "net = MyCNN()\n",
    "\n",
    "\n",
    "# This tells our model to send all of the tensors and operations to the GPU (or keep them at the CPU if we're not using GPU)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"print(type(waldo_dataset))\n",
    "reduced_waldo_dataset = []\n",
    "for i in range(100):\n",
    "  reduced_waldo_dataset.append(waldo_dataset[i])\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"reduced_train_size = int(0.75 * len(reduced_waldo_dataset))\n",
    "reduced_test_size = len(reduced_waldo_dataset) - reduced_train_size\n",
    "reduced_waldo_dataset_train, reduced_waldo_dataset_test = torch.utils.data.random_split(reduced_waldo_dataset, [reduced_train_size, reduced_test_size])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(0.75 * len(waldo_dataset))\n",
    "test_size = len(waldo_dataset) - train_size\n",
    "waldo_dataset_train, waldo_dataset_test = torch.utils.data.random_split(waldo_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(waldo_dataset_train, batch_size=1)\n",
    "test_loader = DataLoader(waldo_dataset_test, batch_size=1)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "learning_rate = 1e-2\n",
    "n_epochs = 1\n",
    "\n",
    "def train_model(net):\n",
    "    \"\"\" Train a the specified network.\n",
    "\n",
    "        Outputs a tuple with the following four elements\n",
    "        train_hist_x: the x-values (batch number) that the training set was \n",
    "            evaluated on.\n",
    "        train_loss_hist: the loss values for the training set corresponding to\n",
    "            the batch numbers returned in train_hist_x\n",
    "        test_hist_x: the x-values (batch number) that the test set was \n",
    "            evaluated on.\n",
    "        test_loss_hist: the loss values for the test set corresponding to\n",
    "            the batch numbers returned in test_hist_x\n",
    "    \"\"\" \n",
    "    loss, optimizer = net.get_loss(learning_rate)\n",
    "    # Define some parameters to keep track of metrics\n",
    "    print_every = 50\n",
    "    idx = 0\n",
    "    #print(\"started training...\")\n",
    "    train_hist_x = []\n",
    "    train_loss_hist = []\n",
    "    test_hist_x = []\n",
    "    test_loss_hist = []\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    # Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        #print(\"epoch %(e)d\" % {\"e\":epoch})\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #print(\"about to enumerate\")\n",
    "        print(len(train_loader))\n",
    "        for i, data in enumerate(train_loader):\n",
    "          #print(i, data)\n",
    "            #print(\"enumerating...\")\n",
    "            # Get inputs in right form\n",
    "           # print(data)\n",
    "            #print(data['image']) #rint(data['inputs'])\n",
    "            inputs, labels = data['image'], data['label']\n",
    "            #print(inputs)\n",
    "          #print(labels)\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n",
    "          \n",
    "            #print(\"initializing optimizer...\")\n",
    "          # In Pytorch, We need to always remember to set the optimizer gradients to 0 before we recompute the new gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #print(\"about to forward pass\")\n",
    "\n",
    "          # Forward pass\n",
    "            #inputs = inputs.unsqueeze(0)\n",
    "            outputs = net(inputs)\n",
    "          \n",
    "            #print(\"forward passed\")\n",
    "          # Compute the loss and find the loss with respect to each parameter of the model\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            #print(\"calculate loss\")\n",
    "          # Change each parameter with respect to the recently computed loss.\n",
    "            optimizer.step()\n",
    "\n",
    "          # Update statistics\n",
    "            running_loss += loss_size.data.item()\n",
    "            #print(\"updated loss\")\n",
    "          # Print every 20th batch of an epoch\n",
    "            if (i % print_every) == print_every-1:\n",
    "                print(\"Epoch {}, Iteration {}\\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                    epoch + 1, i+1,running_loss / print_every, time.time() - start_time))\n",
    "              # Reset running loss and time\n",
    "                train_loss_hist.append(running_loss / print_every)\n",
    "                train_hist_x.append(idx)\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "                idx += 1\n",
    "\n",
    "        # At the end of the epoch, do a pass on the test set\n",
    "            total_test_loss = 0\n",
    "            #print(\"test set pass\")\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                inputs, labels = data['image'], data['label']\n",
    "                #print(\"made it past enumerator\")\n",
    "            # Wrap tensors in Variables\n",
    "                inputs, labels = Variable(inputs).to(device, dtype=torch.float), Variable(labels).to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "                test_outputs = net(inputs)\n",
    "                test_loss_size = loss(test_outputs, labels)\n",
    "                total_test_loss += test_loss_size.data.item()\n",
    "            test_loss_hist.append(total_test_loss / len(test_loader))\n",
    "            test_hist_x.append(idx)\n",
    "            print(\"Validation loss = {:.2f}\".format(\n",
    "                total_test_loss / len(test_loader)))\n",
    "\n",
    "        print(\"Training finished, took {:.2f}s\".format(\n",
    "        time.time() - training_start_time))\n",
    "        return train_hist_x, train_loss_hist, test_hist_x, test_loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Epoch 1, Iteration 50\t train_loss: 0.19 took: 83.69s\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Epoch 1, Iteration 100\t train_loss: 0.02 took: 87.65s\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Epoch 1, Iteration 150\t train_loss: 0.01 took: 88.26s\n",
      "Validation loss = 0.17\n",
      "Validation loss = 0.17\n",
      "Validation loss = 0.17\n",
      "Validation loss = 0.17\n",
      "Validation loss = 0.17\n",
      "Validation loss = 0.17\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Epoch 1, Iteration 200\t train_loss: 0.12 took: 83.03s\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Epoch 1, Iteration 250\t train_loss: 0.19 took: 83.15s\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Epoch 1, Iteration 300\t train_loss: 0.11 took: 84.73s\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Epoch 1, Iteration 350\t train_loss: 0.24 took: 92.31s\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Epoch 1, Iteration 400\t train_loss: 0.04 took: 88.86s\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Epoch 1, Iteration 450\t train_loss: 0.19 took: 87.75s\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Epoch 1, Iteration 500\t train_loss: 0.11 took: 88.08s\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Epoch 1, Iteration 550\t train_loss: 0.18 took: 89.57s\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Epoch 1, Iteration 600\t train_loss: 0.18 took: 87.76s\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Epoch 1, Iteration 650\t train_loss: 0.11 took: 89.07s\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Epoch 1, Iteration 700\t train_loss: 0.01 took: 100.46s\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Epoch 1, Iteration 750\t train_loss: 0.20 took: 96.68s\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.17\n",
      "Validation loss = 0.17\n",
      "Validation loss = 0.18\n",
      "Validation loss = 0.18\n",
      "Validation loss = 0.18\n",
      "Validation loss = 0.18\n",
      "Validation loss = 0.18\n",
      "Validation loss = 0.19\n",
      "Validation loss = 0.18\n",
      "Validation loss = 0.18\n",
      "Validation loss = 0.18\n",
      "Validation loss = 0.18\n",
      "Validation loss = 0.18\n",
      "Validation loss = 0.18\n",
      "Validation loss = 0.17\n",
      "Validation loss = 0.17\n",
      "Validation loss = 0.17\n",
      "Validation loss = 0.17\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.16\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Epoch 1, Iteration 800\t train_loss: 0.32 took: 94.75s\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Epoch 1, Iteration 850\t train_loss: 0.12 took: 99.71s\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Epoch 1, Iteration 900\t train_loss: 0.18 took: 96.98s\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Epoch 1, Iteration 950\t train_loss: 0.03 took: 94.24s\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.15\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Epoch 1, Iteration 1000\t train_loss: 0.20 took: 97.64s\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.13\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Validation loss = 0.14\n",
      "Training finished, took 1879.68s\n"
     ]
    }
   ],
   "source": [
    "train_hist_x, train_loss_hist, test_hist_x, test_loss_hist = train_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: [tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(0, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(0, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(0, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(0, dtype=torch.int32), tensor(0, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(0, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(0, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(0, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(0, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(0, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32), tensor(1, dtype=torch.int32)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d(): argument 'input' (position 1) must be Tensor, not DataLoader",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-6f1de87a7caf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y_true: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwaldo_dataset_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2bc176552b9f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# and returns the output of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not DataLoader"
     ]
    }
   ],
   "source": [
    "#print(\"y_true: \" + str(np.argmawaldo_dataset_test, axis=-1)))\n",
    "print(\"y_true: \" + str([x['label'] for x in waldo_dataset_test]))\n",
    "\n",
    "outputs = net(test_loader)\n",
    "\n",
    "\n",
    "#def acc(y_true, y_pred):\n",
    "#    return np.equal(np.argmax(y_true, axis=-1), np.argmax(y_pred, axis=-1)).mean()\n",
    "\n",
    "#print(\"accuracy: \" + str(acc(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVogThpmZFeKEYWZmhThhmJlZIf8FKtV5odBS8xIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15f90fa15c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right waldos = 29000.0\n",
      "Train accuracy is 0.971899224806\n",
      "right waldos = 10000.0\n",
      "Test accuracy is 0.970930232558\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(net, loader):\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    n_waldo = 0\n",
    "    n_correct_waldo = 0.001\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        # Get inputs in right form\n",
    "        \n",
    "        inputs, labels = data['image'], data['label']\n",
    "        inputs, labels = Variable(inputs).to(device, dtype=torch.float), Variable(labels).to(device, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        if data['label'] == torch.tensor([0], dtype=torch.int, device=device):\n",
    "            n_waldo += 1\n",
    "            n_correct_waldo += np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == labels.cpu().numpy())\n",
    "            #if outputs['label'] == data['label']:\n",
    "            #    n_correct_waldo += 1\n",
    "        #if tensor.get(labels, 0) == 1:\n",
    "        #    print(inputs)\n",
    "        \n",
    "        #for i in outputs:\n",
    "        #   print(np.argmax(outputs.cpu().detach().numpy()))\n",
    "            \n",
    "        \n",
    "        #waldo_correct = for i in(np.argmax(outputs.cpu().detach().numpy(), axis=1) == labels.cpu().numpy()))\n",
    "        n_correct += np.sum(np.argmax(outputs.cpu().detach().numpy(), axis=1) == labels.cpu().numpy())\n",
    "        n_total += labels.shape[0]\n",
    "    \n",
    "    print(\"right waldos = \" + str(n_waldo/n_correct_waldo))\n",
    "    return n_correct/n_total\n",
    "\n",
    "plt.plot(train_hist_x,train_loss_hist)\n",
    "plt.plot(test_hist_x,test_loss_hist)\n",
    "plt.legend(['train loss', 'validation loss'])\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "print(\"Train accuracy is\", get_accuracy(net, train_loader))\n",
    "print(\"Test accuracy is\", get_accuracy(net, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
